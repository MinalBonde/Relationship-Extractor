{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "def REL_embedding(corpus_dict):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    RELSet = set()\n",
    "    for k,v in corpus_dict.items():\n",
    "        if v!=\"Other\":\n",
    "            rel=v.split(\"(\")\n",
    "            relation=rel[0]\n",
    "            RELSet.add(relation)\n",
    "    RELSet.add(\"O\")\n",
    "    li = list(RELSet) \n",
    "    trans = le.fit_transform(li)\n",
    "    \n",
    "    print(trans)\n",
    "    reldict = {}\n",
    "    for i in range(len(li)):\n",
    "        reldict[li[i]] = trans[i]\n",
    "    return reldict\n",
    "# REL_embedding(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "def POS_embedding():\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    set1 = {\"CC\",\"CD\",\"DT\",\"EX\",\"FW\",\"IN\",\"JJ\",\"JJR\",\"JJS\",\"LS\",\"MD\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"PDT\",\"POS\",\"PRP\",\"PRP$\",\"RB\",\"RBR\",\"RBS\",\"RP\",\"TO\",\"UH\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\"}\n",
    "    li = list(set1) \n",
    "    li1 = []\n",
    "    li1.append(li)\n",
    "    trans = le.fit_transform(li)\n",
    "\n",
    "    posEmbDict = {}\n",
    "    for i in range(len(li)):\n",
    "        posEmbDict[li[i]] = trans[i]\n",
    "    return posEmbDict\n",
    "# POS_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "\n",
    "def depParsingEmbedding():\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    set1 = {'ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp','-'}\n",
    "    li = list(set1) \n",
    "    li1 = []\n",
    "    li1.append(li)\n",
    "    trans = le.fit_transform(li)\n",
    "\n",
    "    depParseEmbDict = {}\n",
    "    for i in range(len(li)):\n",
    "        depParseEmbDict[li[i]] = trans[i]\n",
    "    return depParseEmbDict\n",
    "# depParsingEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def NERTagEmbedding():\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    set1 = {\"PERSON\",\"NORP\",\"FAC\",\"ORG\",\"GPE\",\"LOC\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\",\"LAW\",\"LANGUAGE\",\"DATE\",\"TIME\",\"PERCENT\",\"MONEY\",\"QUANTITY\",\"ORDINAL\",\"CARDINAL\",\"NaN\",\"None\"}\n",
    "    li = list(set1) \n",
    "    li1 = []\n",
    "    li1.append(li)\n",
    "    trans = le.fit_transform(li)\n",
    "\n",
    "    nerTagEmbDict = {}\n",
    "    for i in range(len(li)):\n",
    "        nerTagEmbDict[li[i]] = trans[i]\n",
    "    return nerTagEmbDict\n",
    "# NERTagEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def find_hypernyms(word):\n",
    "    hypernyms=[]\n",
    "    for ss in wn.synsets('green'):\n",
    "        for hyper in ss.hypernyms():\n",
    "            hypernyms.append(hyper.name())\n",
    "    return (hypernyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minal\\Anaconda3\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn\n",
    "def Corpus_dictFormation(f):\n",
    "    d=(f.read())\n",
    "    data=d.split(\"\\n\\n\\n\")\n",
    "    data=data[:len(data)-1]\n",
    "    import re\n",
    "    for i in range(len(data)):\n",
    "        data[i]=re.sub(r\"(\\d+\\t)\",\" \",data[i])\n",
    "        data[i]=re.sub('\"',' ',data[i])\n",
    "\n",
    "    corpus_dict={}\n",
    "    for i in range(len(data)):\n",
    "    #     print(i)\n",
    "        temp=data[i].split(\"\\n\")\n",
    "\n",
    "        corpus_dict[temp[0]]=temp[1]\n",
    "    return corpus_dict\n",
    "\n",
    "def Corpus_Reading(corpus_dict):\n",
    "\n",
    "    ind=0\n",
    "    MainDict1={}\n",
    "    relList = []\n",
    "    dirList = []\n",
    "    wholeRelList = []\n",
    "    for k,v in corpus_dict.items():\n",
    "        wholeRelList.append(v)\n",
    "        if ind not in MainDict1.keys():\n",
    "            MainDict1[ind]={'token':{},'POSTAG':{},'NER':{},'DEPPARSE':{},'Direction':\"\",\"REL\":\"\",\"e1Start\":0,\"e1End\":0,\"e2Start\":0,\"e2End\":0}\n",
    "            if v!=\"Other\":\n",
    "                rel=v.split(\"(\")\n",
    "                rel1=rel[0]\n",
    "                relList.append(rel1)\n",
    "                dire=rel[1].split(\",\")\n",
    "                if dire[0]==\"e1\":\n",
    "                    MainDict1[ind][\"Direction\"]=\"+1\"\n",
    "                    MainDict1[ind][\"REL\"]=rel1\n",
    "                    dirList.append(1)\n",
    "                else:\n",
    "                    MainDict1[ind][\"Direction\"]=\"-1\"\n",
    "                    MainDict1[ind][\"REL\"]=rel1\n",
    "                    dirList.append(-1)\n",
    "            else:\n",
    "                MainDict1[ind][\"Direction\"]=\"0\"\n",
    "                MainDict1[ind][\"REL\"]=\"O\"\n",
    "                relList.append(v)\n",
    "                dirList.append(0)\n",
    "\n",
    "\n",
    "            tempSent=k\n",
    "            tempSent=re.sub('<e1>|</e1>|<e2>|</e2>|\"',\"\",tempSent).lstrip()\n",
    "            tempSent=re.sub('-|/',\" \",tempSent)\n",
    "\n",
    "            tokens=nltk.word_tokenize(tempSent)\n",
    "            for i in range(len(tokens)):\n",
    "                MainDict1[ind][\"token\"][i]=tokens[i]\n",
    "\n",
    "            nlpdoc = nlp(tempSent)\n",
    "            for nd in nlpdoc:\n",
    "        #         print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "                if nd.text.isspace()==False and nd.text in tokens:\n",
    "                    MainDict1[ind]['DEPPARSE'][tokens.index(nd.text)]=nd.dep_\n",
    "\n",
    "\n",
    "            e1=re.findall(\"<e1>.*?</e1>\",k)\n",
    "            e2=re.findall(\"<e2>.*?</e2>\",k)\n",
    "            e1=re.sub(\"<e1>|</e1>\",\"\",e1[0])\n",
    "      \n",
    "            e1=re.sub(r\"([a-z])\\-([a-z])\", r\"\\1 \\2\", e1 , 0, re.IGNORECASE)\n",
    "            e2=re.sub(\"<e2>|</e2>\",\"\",e2[0])\n",
    "       \n",
    "            e2=re.sub(r\"([a-z])\\-([a-z])\", r\"\\1 \\2\", e2 , 0, re.IGNORECASE)\n",
    "            e1_list=e1.split(\" \")\n",
    "            e2_list=e2.split(\" \")\n",
    "    \n",
    "            MainDict1[ind][\"e1Start\"]=tokens.index(e1_list[0])\n",
    "            MainDict1[ind][\"e1End\"]=tokens.index(e1_list[-1])\n",
    "            MainDict1[ind][\"e2Start\"]=tokens.index(e2_list[0])\n",
    "            MainDict1[ind][\"e2End\"]=tokens.index(e2_list[-1])\n",
    "        \n",
    "            posTags=nltk.pos_tag(tokens)\n",
    "            for i in range(len(tokens)):\n",
    "                MainDict1[ind][\"POSTAG\"][i]=posTags[i][1]\n",
    "\n",
    "\n",
    "    \n",
    "            spacy_sent1=\" \".join(e1_list)\n",
    "            spacy_sent2=\" \".join(e2_list)\n",
    "   \n",
    "            doc1=nlp(spacy_sent1)\n",
    "            doc2=nlp(spacy_sent2)\n",
    "   \n",
    "            l1,l2=0,0\n",
    "            for i in tokens:\n",
    "                    MainDict1[ind]['NER'][tokens.index(i)]=\"None\"\n",
    "\n",
    "            for e1 in doc1.ents:\n",
    "                l1=e1.label_\n",
    "\n",
    "            for e2 in doc2.ents:\n",
    "                l2=e2.label_\n",
    "\n",
    "\n",
    "            for i in tokens:\n",
    "                if i in e1_list and MainDict1[ind]['NER'][tokens.index(i)]==\"None\":\n",
    "                    if l1==0:\n",
    "                        MainDict1[ind]['NER'][tokens.index(i)]=\"NaN\"\n",
    "                    else:\n",
    "                        MainDict1[ind]['NER'][tokens.index(i)]=l1\n",
    "                elif i in e2_list and MainDict1[ind]['NER'][tokens.index(i)]==\"None\":\n",
    "                    if l2==0:\n",
    "                        MainDict1[ind]['NER'][tokens.index(i)]=\"NaN\"\n",
    "                    else:\n",
    "                        MainDict1[ind]['NER'][tokens.index(i)]=l2\n",
    "\n",
    "            tempSent=\"\"  \n",
    "            ind+=1\n",
    "    MainDict2={}\n",
    "    ind=0\n",
    "\n",
    "    for k,v in corpus_dict.items():\n",
    "        if ind not in MainDict2.keys():\n",
    "            MainDict2[ind]={'token':[],'POSTAG':[],'NER':[],'DEPPARSE':[],'Direction':\"\",\"REL\":\"\",\"e1Start\":0,\"e1End\":0,\"e2Start\":0,\"e2End\":0,\"lemma\":[]}\n",
    "\n",
    "            for k1,v1 in MainDict1[ind]['token'].items():\n",
    "                 MainDict2[ind]['token'].append(v1)\n",
    "\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            for i in MainDict2[ind]['token']:\n",
    "                MainDict2[ind]['lemma'].append(lemmatizer.lemmatize(i))\n",
    "                \n",
    "#             for i in MainDict2[ind]['token']:\n",
    "#                 tempHyper=find_hypernyms(i) \n",
    "#                 MainDict2[ind]['hypernyms'].append(tempHyper)\n",
    "                \n",
    "            for k1,v1 in MainDict1[ind]['POSTAG'].items():\n",
    "                MainDict2[ind]['POSTAG'].append(v1)\n",
    "            for k1,v1 in MainDict1[ind]['NER'].items():\n",
    "                MainDict2[ind]['NER'].append(v1)\n",
    "            for k1,v1 in MainDict1[ind]['DEPPARSE'].items():\n",
    "                MainDict2[ind]['DEPPARSE'].append(v1)\n",
    "            MainDict2[ind]['REL']=MainDict1[ind]['REL']\n",
    "\n",
    "            MainDict2[ind]['Direction']=int(MainDict1[ind]['Direction'])\n",
    "            MainDict2[ind]['e1Start']=int(MainDict1[ind]['e1Start'])\n",
    "            MainDict2[ind]['e1End']=int(MainDict1[ind]['e1End'])\n",
    "            MainDict2[ind]['e2Start']=int(MainDict1[ind]['e2Start'])\n",
    "            MainDict2[ind]['e2End']=int(MainDict1[ind]['e2End'])\n",
    "            ind+=1\n",
    "            \n",
    "    MainDictEmbedding={}\n",
    "    ind=0\n",
    "    nerTagEmbDict=NERTagEmbedding()\n",
    "    depParseEmbDict=depParsingEmbedding()\n",
    "    posEmbDict=POS_embedding()\n",
    "    reldict=REL_embedding(corpus_dict)\n",
    "\n",
    "    for k,v in corpus_dict.items():\n",
    "        if ind not in MainDictEmbedding.keys():\n",
    "            MainDictEmbedding[ind]={'token':[],'POSTAG':[],'NER':[],'DEPPARSE':[],'Direction':\"\",\"REL\":\"\",\"e1Start\":0,\"e1End\":0,\"e2Start\":0,\"e2End\":0,\"lemma\":[]}\n",
    "\n",
    "            tempSent=k\n",
    "            tempSent=re.sub('<e1>|</e1>|<e2>|</e2>|\"',\"\",tempSent).lstrip()\n",
    "            tempSent=re.sub('-|/',\" \",tempSent)\n",
    "\n",
    "            tokens=nltk.word_tokenize(tempSent)\n",
    "            for i in range(len(tokens)):\n",
    "                MainDict1[ind][\"token\"][i]=tokens[i]\n",
    "            tokenli=[]\n",
    "            tokenli.append(tokens)\n",
    "            model = Word2Vec(tokenli, min_count=1,size=1)\n",
    "            words = list(model.wv.vocab)\n",
    "            for i in words:\n",
    "                if i in tokens:\n",
    "                    (MainDictEmbedding[ind]['token']).append(int(model[i][0]*1000))\n",
    "            MainDictEmbedding[ind]['token']=np.std(MainDictEmbedding[ind]['token'])\n",
    "            \n",
    "#             hyperEmb=MainDict2[ind]['hypernyms']\n",
    "#             model2 = Word2Vec(hyperEmb, min_count=1,size=1)\n",
    "#             words2 = list(model2.wv.vocab)\n",
    "#             for i in words2:\n",
    "#                 if i in hyperEmb:\n",
    "#                     (MainDictEmbedding[ind]['hypernyms']).append(int(np.mean(model2[i][0]*1000)))\n",
    "#             MainDictEmbedding[ind]['hypernyms']=np.mean(MainDictEmbedding[ind]['hypernyms'])\n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            tempLemma=[]\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            for i in tokens:\n",
    "                tempLemma.append(lemmatizer.lemmatize(i))\n",
    "            lemmaLi=[]\n",
    "            lemmaLi.append(tempLemma)\n",
    "            model1 = Word2Vec(lemmaLi, min_count=1,size=1)\n",
    "            words1 = list(model1.wv.vocab)\n",
    "            for i in words1:\n",
    "                if i in tempLemma:\n",
    "                    (MainDictEmbedding[ind]['lemma']).append(int(model1[i][0]*1000))\n",
    "            MainDictEmbedding[ind]['lemma']=np.std(MainDictEmbedding[ind]['lemma'])\n",
    "            \n",
    "            \n",
    "\n",
    "            for k1,v1 in MainDict1[ind]['POSTAG'].items():\n",
    "                if v1 in posEmbDict.keys():\n",
    "                    (MainDictEmbedding[ind]['POSTAG']).append(posEmbDict[v1])\n",
    "                else:\n",
    "                    (MainDictEmbedding[ind]['POSTAG']).append(0)\n",
    "            MainDictEmbedding[ind]['POSTAG']=np.std(MainDictEmbedding[ind]['POSTAG'])\n",
    "\n",
    "            for k1,v1 in MainDict1[ind]['NER'].items():\n",
    "                if v1 in nerTagEmbDict.keys():\n",
    "                    (MainDictEmbedding[ind]['NER']).append(nerTagEmbDict[v1])\n",
    "                else:\n",
    "                    (MainDictEmbedding[ind]['NER']).append(0)\n",
    "            MainDictEmbedding[ind]['NER']=np.std(MainDictEmbedding[ind]['NER'])\n",
    "\n",
    "            for k1,v1 in MainDict1[ind]['DEPPARSE'].items():\n",
    "                if v1 in depParseEmbDict.keys():\n",
    "                    (MainDictEmbedding[ind]['DEPPARSE']).append(depParseEmbDict[v1])\n",
    "                else:\n",
    "                    (MainDictEmbedding[ind]['DEPPARSE']).append(0)\n",
    "            MainDictEmbedding[ind]['DEPPARSE']=np.std(MainDictEmbedding[ind]['DEPPARSE'])        \n",
    "\n",
    "            if MainDict1[ind]['REL'] in reldict:\n",
    "                MainDictEmbedding[ind]['REL']=reldict[MainDict1[ind]['REL']]\n",
    "\n",
    "            MainDictEmbedding[ind]['Direction']=int(MainDict1[ind]['Direction'])\n",
    "            MainDictEmbedding[ind]['e1Start']=int(MainDict1[ind]['e1Start'])\n",
    "            MainDictEmbedding[ind]['e1End']=int(MainDict1[ind]['e1End'])\n",
    "            MainDictEmbedding[ind]['e2Start']=int(MainDict1[ind]['e2Start'])\n",
    "            MainDictEmbedding[ind]['e2End']=int(MainDict1[ind]['e2End'])\n",
    "            ind+=1\n",
    "            \n",
    "    \n",
    "    df= pd.DataFrame.from_dict(MainDictEmbedding)\n",
    "\n",
    "    return(df,relList,dirList,wholeRelList)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 6 0 7 8 5 4 9 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:181: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\minal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:205: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 6 0 7 8 5 4 9 2]\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk\n",
    "import time\n",
    "\n",
    "start_time= time.time()\n",
    "f2=open(r\"test_data1.txt\")\n",
    "f1=open(r\"train_data1.txt\")\n",
    "corpus_dict1=Corpus_dictFormation(f1)\n",
    "corpus_dict2=Corpus_dictFormation(f2)\n",
    "\n",
    "df1,relList1,dirList1,wholeRelList1=Corpus_Reading(corpus_dict1)\n",
    "df2,relList2,dirList2,wholeRelList2=Corpus_Reading(corpus_dict2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minal\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\minal\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3070692194403535\n",
      "[[7.]\n",
      " [9.]\n",
      " [5.]\n",
      " ...\n",
      " [9.]\n",
      " [1.]\n",
      " [9.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "Train_X = np.asarray(df1.loc[['POSTAG', 'NER', 'DEPPARSE','Direction','lemma']])\n",
    "Train_Y = np.atleast_2d(np.asarray(df1.loc['REL']))\n",
    "Train_X=Train_X.T\n",
    "Train_Y=Train_Y.T\n",
    "# Train_Y=Train_Y.astype('int')\n",
    "\n",
    "Test_X = np.asarray(df2.loc[['POSTAG', 'NER', 'DEPPARSE','Direction','lemma']])\n",
    "Test_Y = np.atleast_2d(np.asarray(df2.loc['REL']))\n",
    "Test_X=Test_X.T\n",
    "Test_Y=Test_Y.T\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(Train_X,Train_Y)\n",
    "\n",
    "y_predict=np.atleast_2d(clf.predict(Test_X))\n",
    "# print(l)\n",
    "# print(y_predict)\n",
    "\n",
    "# print(clf.score(l,Test_Y.T))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Test_Y, y_predict.T))\n",
    "print(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 6 0 7 8 5 4 9 2]\n",
      "2716\n"
     ]
    }
   ],
   "source": [
    "relDict = REL_embedding(corpus_dict1)\n",
    "count =0\n",
    "for v1 in y_predict[0]:\n",
    "    for k,v in relDict.items():\n",
    "        if v1 == v:\n",
    "#             print(k)\n",
    "            count +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2716\n"
     ]
    }
   ],
   "source": [
    "relList2Encode =[]\n",
    "for l in relList2:\n",
    "    if l in relDict.keys():\n",
    "        relList2Encode.append(relDict[l])\n",
    "    else:\n",
    "        relList2Encode.append(10)\n",
    "        \n",
    "# print(relList2Encode)\n",
    "print(len(relList2Encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17268041237113402"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(relList2Encode, y_predict.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minal\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\minal\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0. -1. ... -1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "Train_X1 = np.asarray(df1.loc[['POSTAG', 'NER', 'DEPPARSE','REL','lemma']])\n",
    "Train_Y1 = np.atleast_2d(np.asarray(df1.loc['Direction']))\n",
    "Train_X1=Train_X1.T\n",
    "Train_Y1=Train_Y1.T\n",
    "\n",
    "\n",
    "Test_X1 = np.asarray(df2.loc[['POSTAG', 'NER', 'DEPPARSE','REL','lemma']])\n",
    "Test_Y1 = np.atleast_2d(np.asarray(df2.loc['Direction']))\n",
    "Test_X1=Test_X1.T\n",
    "Test_Y1=Test_Y1.T\n",
    "\n",
    "clf1 = svm.SVC()\n",
    "clf1.fit(Train_X1,Train_Y1)\n",
    "\n",
    "y_predict1=np.atleast_2d(clf1.predict(Test_X1))\n",
    "# print(l)\n",
    "print(y_predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17268041237113402\n"
     ]
    }
   ],
   "source": [
    "count1=0\n",
    "predictedRelationList = []\n",
    "for v1 in range(len(y_predict[0])):\n",
    "    for k,v in relDict.items():\n",
    "        if y_predict[0][v1] == v:\n",
    "            predictedRelationList.append((k))\n",
    "            \n",
    "        elif y_predict[0][v1] == v :\n",
    "            predictedRelationList.append(k)\n",
    "            \n",
    "        elif y_predict[0][v1] == v:\n",
    "            predictedRelationList.append(k)\n",
    "            \n",
    "onlyrel=[]\n",
    "for i in wholeRelList2:\n",
    "    if i!=\"O\":\n",
    "        onlyrel.append(i.split(\"(\")[0])\n",
    "    else:\n",
    "        onlyrel.append(\"O\")\n",
    "        \n",
    "\n",
    "for i in range(len(wholeRelList2)):\n",
    "    if onlyrel[i]==predictedRelationList[i]:\n",
    "        count1+=1\n",
    "#         print(\"1->\",i)\n",
    "#         print(\"2->\",onlyrel[i])\n",
    "print(count1/len(predictedRelationList))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15721649484536082\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# print(accuracy_score(dirList2, y_predict1.T))\n",
    "count=0\n",
    "predictedRelList = []\n",
    "for v1 in range(len(y_predict[0])):\n",
    "    for k,v in relDict.items():\n",
    "        if y_predict[0][v1] == v and y_predict1[0][v1] == 1:\n",
    "            predictedRelList.append(str(k+\"(e1,e2)\"))\n",
    "            \n",
    "        elif y_predict[0][v1] == v and y_predict1[0][v1] == -1:\n",
    "            predictedRelList.append(str(k+\"(e2,e1)\"))\n",
    "            \n",
    "        elif y_predict[0][v1] == v and y_predict1[0][v1] == 0:\n",
    "            predictedRelList.append(k)\n",
    "            \n",
    "\n",
    "        \n",
    "for i in range(len(predictedRelList)):\n",
    "    if(predictedRelList[i] == wholeRelList2[i]):\n",
    "        count +=1\n",
    "#         print(\"1->\",i)\n",
    "#         print(\"2->\",predictedRelList[i])\n",
    "print(count/len(predictedRelList))        \n",
    "# print(predictedRelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n"
     ]
    }
   ],
   "source": [
    "# x=[]\n",
    "# for i in ['dog']:\n",
    "#     y=find_hypernyms(i) \n",
    "#     x.append(y)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n",
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n",
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n",
      "[['chromatic_color.n.01', 'tract.n.01', 'environmentalist.n.01', 'site.n.01', 'vegetable.n.01', 'ketamine.n.01', 'discolor.v.03']]\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "# hyperEmb1=x\n",
    "# res=[]\n",
    "\n",
    "# # print(hyperEmb1)\n",
    "# for i in hyperEmb1:\n",
    "#     some=[]\n",
    "    \n",
    "#     some.append(i)\n",
    "#     print(some)\n",
    "#     modelh = Word2Vec(some, min_count=1,size=1)\n",
    "#     wordsh = list(modelh.wv.vocab)\n",
    "#     for i in wordsh:\n",
    "#         if i in hyperEmb1:\n",
    "#             res.append(int(np.mean(modelh[i][0]*1000)))\n",
    "# ans=np.mean(res)\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312.3829083442688\n"
     ]
    }
   ],
   "source": [
    "print(time.time()- start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24759391816121717"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(Test_Y,y_predict.T, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22482597720135516"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(Test_Y,y_predict.T, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2362411944704018"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(Test_Y,y_predict.T, average='macro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
